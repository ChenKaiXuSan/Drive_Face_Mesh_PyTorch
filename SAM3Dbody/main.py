#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
File: /workspace/code/SAM3Dbody/main_multi_gpu_process.py
Project: /workspace/code/SAM3Dbody
Created Date: Monday January 26th 2026
Author: Kaixu Chen
-----
Comment:
æ ¹æ®å¤šGPUå¹¶è¡Œå¤„ç†SAM-3D-Bodyæ¨ç†ä»»åŠ¡ã€‚

Have a good code time :)
-----
Last Modified: Monday January 26th 2026 5:12:10 pm
Modified By: the developer formerly known as Kaixu Chen at <chenkaixusan@gmail.com>
-----
Copyright (c) 2026 The University of Tsukuba
-----
HISTORY:
Date      	By	Comments
----------	---	---------------------------------------------------------
"""

import logging
import os
import multiprocessing as mp
from pathlib import Path
from typing import Dict, List
import numpy as np

import hydra
from omegaconf import DictConfig, OmegaConf

# å‡è®¾è¿™äº›æ˜¯ä»ä½ çš„å…¶ä»–æ¨¡å—å¯¼å…¥çš„
from .infer import process_frame_list
from .load import load_data

# --- å¸¸é‡å®šä¹‰ ---
REQUIRED_VIEWS = {"front", "left", "right"}

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------
# æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼šå¤„ç†å•ä¸ªäººçš„æ•°æ®
# ---------------------------------------------------------------------
def process_single_person(
    person_dir: Path,
    source_root: Path,
    out_root: Path,
    infer_root: Path,
    cfg: DictConfig,
):
    """å¤„ç†å•ä¸ªäººå‘˜çš„æ‰€æœ‰ç¯å¢ƒå’Œè§†è§’"""
    person_id = person_dir.name
    vid_patterns = ["*.mp4", "*.mov", "*.avi", "*.mkv", "*.MP4", "*.MOV"]

    # --- 1. Personå°‚ç”¨ã®ãƒ­ã‚°è¨­å®š ---
    log_dir = out_root / "person_logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    person_log_file = log_dir / f"{person_id}.log"

    # æ–°ã—ã„ãƒãƒ³ãƒ‰ãƒ©ã‚’ä½œæˆ
    handler = logging.FileHandler(person_log_file, mode="a", encoding="utf-8")
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    handler.setFormatter(formatter)

    logger = logging.getLogger(person_id)  # ã“ã®Personå°‚ç”¨ã®ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—
    logger.addHandler(handler)
    logger.propagate = False  # è¦ªï¼ˆRootï¼‰ãƒ­ã‚¬ãƒ¼ã«ãƒ­ã‚°ã‚’æµã•ãªã„ï¼ˆæ··ã–ã‚‹ã®ã‚’é˜²ãï¼‰

    logger.info(f"==== Starting Process for Person: {person_id} ====")

    env_dirs = sorted([x for x in person_dir.iterdir() if x.is_dir()])
    if not env_dirs:
        logger.warning(f"è·³è¿‡ï¼š{person_dir} ä¸‹æ²¡æœ‰ç¯å¢ƒç›®å½•")
        return

    for env_dir in env_dirs:
        env_name = env_dir.name
        rel_env = env_dir.relative_to(source_root)

        # --- è§†é¢‘å¤„ç†é€»è¾‘ ---
        view_map: Dict[str, Path] = {}
        for pat in vid_patterns:
            for f in env_dir.glob(pat):
                stem = f.stem.lower()
                if stem in REQUIRED_VIEWS:
                    view_map[stem] = f.resolve()

        if not all(v in view_map for v in REQUIRED_VIEWS):
            logger.warning(f"[Skip] {rel_env}: è§†è§’ä¸å…¨ {list(view_map.keys())}")
            continue

        view_frames: Dict[str, List[np.ndarray]] = load_data(view_map)

        for view, frames in view_frames.items():
            logger.info(f"  è§†è§’ {view} å¤„ç†äº† {len(frames)} å¸§æ•°æ®ã€‚")
            _out_root = out_root / rel_env / view
            _out_root.mkdir(parents=True, exist_ok=True)
            _infer_root = infer_root / rel_env / view
            _infer_root.mkdir(parents=True, exist_ok=True)

            process_frame_list(
                frame_list=frames,
                out_dir=_out_root,
                inference_output_path=_infer_root,
                cfg=cfg,
            )


# ---------------------------------------------------------------------
# GPU Workerï¼šè¿›ç¨‹æ‰§è¡Œå‡½æ•°
# ---------------------------------------------------------------------
def gpu_worker(
    gpu_id: int,
    person_dirs: List[Path],
    source_root: Path,
    out_root: Path,
    infer_root: Path,
    cfg_dict: dict,
):
    """
    æ¯ä¸ªè¿›ç¨‹çš„å…¥å£ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¹¶å¤„ç†åˆ†é…çš„ä»»åŠ¡åˆ—è¡¨
    """
    # 1. éš”ç¦» GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)

    cfg_dict["infer"]["gpu"] = 0  # å› ä¸ºä¸Šé¢å·²ç»éš”ç¦»äº† GPUï¼Œæ‰€ä»¥è¿™é‡Œè®¾ä¸º 0

    # 2. å°†å­—å…¸è½¬å› Hydra é…ç½®ï¼ˆå¤šè¿›ç¨‹ä¼ é€’å¯¹è±¡æ—¶ï¼Œè½¬ä¸ºå­—å…¸æ›´å®‰å…¨ï¼‰
    cfg = OmegaConf.create(cfg_dict)

    logger.info(f"ğŸŸ¢ GPU {gpu_id} è¿›ç¨‹å¯åŠ¨ï¼Œå¾…å¤„ç†äººæ•°: {len(person_dirs)}")

    for p_dir in person_dirs:
        try:
            process_single_person(p_dir, source_root, out_root, infer_root, cfg)
        except Exception as e:
            logger.error(f"âŒ GPU {gpu_id} å¤„ç† {p_dir.name} æ—¶å‡ºé”™: {e}")

    logger.info(f"ğŸ GPU {gpu_id} æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•")


# ---------------------------------------------------------------------
# Main å…¥å£
# ---------------------------------------------------------------------
# @hydra.main(config_path="../configs", config_name="sam3d_body", version_base=None)
# def main(cfg: DictConfig) -> None:
#     # 1. è·¯å¾„å‡†å¤‡
#     out_root = Path(cfg.paths.log_path).resolve()
#     infer_root = Path(cfg.paths.result_output_path).resolve()
#     source_root = Path(cfg.paths.video_path).resolve()

#     gpu_ids = cfg.infer.get("gpu", [0, 1])  # ä»é…ç½®æ–‡ä»¶è¯»å– GPU åˆ—è¡¨ï¼Œé»˜è®¤ [0, 1]

#     all_person_dirs = sorted([x for x in source_root.iterdir() if x.is_dir()])
#     if not all_person_dirs:
#         logger.error(f"æœªæ‰¾åˆ°æ•°æ®ç›®å½•: {source_root}")
#         return

#     # 2. è‡ªåŠ¨åˆ†ç»„é€»è¾‘ (Task Chunking)
#     # å°†æ‰€æœ‰ç›®å½•åˆ†æˆ N ä»½ï¼ŒN ç­‰äº GPU çš„æ•°é‡
#     num_gpus = len(gpu_ids)
#     # ä½¿ç”¨ np.array_split å¯ä»¥ç¡®ä¿å³ä½¿é™¤ä¸å°½ï¼Œåˆ†é…ä¹Ÿå°½å¯èƒ½å‡åŒ€
#     chunks = np.array_split(all_person_dirs, num_gpus)

#     logger.info(f"æ£€æµ‹åˆ° {num_gpus} ä¸ª GPU: {gpu_ids}")
#     for i, gpu_id in enumerate(gpu_ids):
#         logger.info(f"  - GPU {gpu_id} åˆ†é…ä»»åŠ¡æ•°: {len(chunks[i])}")

#     # 3. å¯åŠ¨å¹¶è¡Œè¿›ç¨‹
#     cfg_dict = OmegaConf.to_container(cfg, resolve=True)
#     mp.set_start_method("spawn", force=True)

#     processes = []
#     for i, gpu_id in enumerate(gpu_ids):
#         person_list = chunks[i].tolist()  # è½¬å›æ™®é€šåˆ—è¡¨
#         if not person_list:
#             continue

#         p = mp.Process(
#             target=gpu_worker,
#             args=(
#                 gpu_id,
#                 person_list,
#                 source_root,
#                 out_root,
#                 infer_root,
#                 cfg_dict,
#             ),
#         )
#         p.start()
#         processes.append(p)

#     # 4. ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
#     for p in processes:
#         p.join()

#     logger.info("ğŸ‰ [SUCCESS] æ‰€æœ‰ GPU ä»»åŠ¡å·²åœ†æ»¡å®Œæˆï¼")


# ---------------------------------------------------------------------
# Main å…¥å£
# ---------------------------------------------------------------------
@hydra.main(config_path="../configs", config_name="sam3d_body", version_base=None)
def main(cfg: DictConfig) -> None:
    # 1. çµŒè·¯æº–å‚™
    out_root = Path(cfg.paths.log_path).resolve()
    infer_root = Path(cfg.paths.result_output_path).resolve()
    source_root = Path(cfg.paths.video_path).resolve()

    # --- è¨­å®šã®è¿½åŠ  ---
    gpu_ids = cfg.infer.get("gpu", [0, 1])  # ä½¿ç”¨ã™ã‚‹GPUã®ãƒªã‚¹ãƒˆ
    workers_per_gpu = cfg.infer.get("workers_per_gpu", 2)  # 1æšã‚ãŸã‚Šã®ãƒ—ãƒ­ã‚»ã‚¹æ•°
    
    # å®Ÿéš›ã«èµ·å‹•ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã®æ•°ã ã‘GPU IDã‚’ä¸¦ã¹ã‚‹ (ä¾‹: [0, 0, 1, 1])
    expanded_gpu_ids = []
    for gid in gpu_ids:
        expanded_gpu_ids.extend([gid] * workers_per_gpu)
    
    total_workers = len(expanded_gpu_ids)
    # ------------------

    all_person_dirs = sorted([x for x in source_root.iterdir() if x.is_dir()])
    if not all_person_dirs:
        logger.error(f"æœªæ‰¾åˆ°æ•°æ®ç›®å½•: {source_root}")
        return

    # 2. è‡ªå‹•åˆ†ç»„é€»è¾‘ (ãƒ—ãƒ­ã‚»ã‚¹ã®ç·æ•°ã§åˆ†å‰²)
    chunks = np.array_split(all_person_dirs, total_workers)

    logger.info(f"ä½¿ç”¨ GPU: {gpu_ids} (å„ {workers_per_gpu} ãƒ¯ãƒ¼ã‚«ãƒ¼)")
    logger.info(f"ç·ãƒ—ãƒ­ã‚»ã‚¹æ•°: {total_workers}")

    # 3. å¯åŠ¨å¹¶è¡Œè¿›ç¨‹
    cfg_dict = OmegaConf.to_container(cfg, resolve=True)
    mp.set_start_method("spawn", force=True)

    processes = []
    for i, gpu_id in enumerate(expanded_gpu_ids):
        person_list = chunks[i].tolist()
        if not person_list:
            continue

        logger.info(f"  - Worker {i} (GPU {gpu_id}) åˆ†é…ä»»åŠ¡æ•°: {len(person_list)}")

        p = mp.Process(
            target=gpu_worker,
            args=(
                gpu_id,
                person_list,
                source_root,
                out_root,
                infer_root,
                cfg_dict,
            ),
        )
        p.start()
        processes.append(p)

    # 4. ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    for p in processes:
        p.join()

    logger.info("ğŸ‰ [SUCCESS] æ‰€æœ‰ GPU ä»»åŠ¡å·²åœ†æ»¡å®Œæˆï¼")

if __name__ == "__main__":
    os.environ["HYDRA_FULL_ERROR"] = "1"
    main()
