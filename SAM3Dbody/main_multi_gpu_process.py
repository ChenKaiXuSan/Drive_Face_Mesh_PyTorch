import logging
import os
import multiprocessing as mp
from pathlib import Path
from typing import Dict, List
import numpy as np

import hydra
from omegaconf import DictConfig, OmegaConf

# å‡è®¾è¿™äº›æ˜¯ä»ä½ çš„å…¶ä»–æ¨¡å—å¯¼å…¥çš„
from .infer import process_frame_list
from .load import load_data

logger = logging.getLogger(__name__)

# --- å¸¸é‡å®šä¹‰ ---
VALID_VIEWS = {"front", "left", "right"}
REQUIRED_VIEWS = {"front", "left", "right"}


# ---------------------------------------------------------------------
# æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼šå¤„ç†å•ä¸ªäººçš„æ•°æ®
# ---------------------------------------------------------------------
def process_single_person(
    person_dir: Path,
    source_root: Path,
    out_root: Path,
    infer_root: Path,
    cfg: DictConfig,
    infer_type: str,
):
    """å¤„ç†å•ä¸ªäººå‘˜çš„æ‰€æœ‰ç¯å¢ƒå’Œè§†è§’"""
    person_id = person_dir.name
    vid_patterns = ["*.mp4", "*.mov", "*.avi", "*.mkv", "*.MP4", "*.MOV"]
    img_patterns = ["*.png", "*.jpg", "*.jpeg", "*.PNG", "*.JPG", "*.JPEG"]

    env_dirs = sorted([x for x in person_dir.iterdir() if x.is_dir()])
    if not env_dirs:
        logger.warning(f"è·³è¿‡ï¼š{person_dir} ä¸‹æ²¡æœ‰ç¯å¢ƒç›®å½•")
        return

    for env_dir in env_dirs:
        env_name = env_dir.name
        rel_env = env_dir.relative_to(source_root)

        # --- è§†é¢‘å¤„ç†é€»è¾‘ ---
        if infer_type == "video":
            view_map = {}
            for pat in vid_patterns:
                for f in env_dir.glob(pat):
                    stem = f.stem.lower()
                    if stem in VALID_VIEWS:
                        view_map[stem] = f.resolve()

            if not all(v in view_map for v in REQUIRED_VIEWS):
                logger.warning(f"[Skip] {rel_env}: è§†è§’ä¸å…¨ {list(view_map.keys())}")
                continue

            file_list = [view_map[v] for v in ("front", "left", "right")]
            frame_list = load_data(file_list)

            _execute_inference(
                frame_list, out_root / rel_env, infer_root / rel_env, cfg
            )

        # --- å›¾åƒå¤„ç†é€»è¾‘ ---
        else:
            for view_name in REQUIRED_VIEWS:
                view_dir = env_dir / view_name
                if not view_dir.exists():
                    continue

                files = []
                for pat in img_patterns:
                    files.extend(view_dir.glob(pat))
                files = sorted(list(set(files)))

                if not files:
                    continue

                rel_view = view_dir.relative_to(source_root)
                frame_list = load_data(files)

                _execute_inference(
                    frame_list, out_root / rel_view, infer_root / rel_view, cfg
                )


def _execute_inference(frame_list, out_dir, infer_dir, cfg):
    """æ‰§è¡Œæ¨æ–­çš„è¾…åŠ©å‡½æ•°"""
    out_dir.mkdir(parents=True, exist_ok=True)
    infer_dir.mkdir(parents=True, exist_ok=True)
    process_frame_list(
        frame_list=frame_list,
        out_dir=out_dir,
        inference_output_path=infer_dir,
        cfg=cfg,
    )


# ---------------------------------------------------------------------
# GPU Workerï¼šè¿›ç¨‹æ‰§è¡Œå‡½æ•°
# ---------------------------------------------------------------------
def gpu_worker(
    gpu_id: int,
    person_dirs: List[Path],
    source_root: Path,
    out_root: Path,
    infer_root: Path,
    cfg_dict: dict,
    infer_type: str,
):
    """
    æ¯ä¸ªè¿›ç¨‹çš„å…¥å£ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¹¶å¤„ç†åˆ†é…çš„ä»»åŠ¡åˆ—è¡¨
    """
    # 1. éš”ç¦» GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)

    # 2. å°†å­—å…¸è½¬å› Hydra é…ç½®ï¼ˆå¤šè¿›ç¨‹ä¼ é€’å¯¹è±¡æ—¶ï¼Œè½¬ä¸ºå­—å…¸æ›´å®‰å…¨ï¼‰
    cfg = OmegaConf.create(cfg_dict)

    logger.info(f"ğŸŸ¢ GPU {gpu_id} è¿›ç¨‹å¯åŠ¨ï¼Œå¾…å¤„ç†äººæ•°: {len(person_dirs)}")

    for p_dir in person_dirs:
        try:
            process_single_person(
                p_dir, source_root, out_root, infer_root, cfg, infer_type
            )
        except Exception as e:
            logger.error(f"âŒ GPU {gpu_id} å¤„ç† {p_dir.name} æ—¶å‡ºé”™: {e}")

    logger.info(f"ğŸ GPU {gpu_id} æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•")


# ---------------------------------------------------------------------
# Main å…¥å£
# ---------------------------------------------------------------------
@hydra.main(config_path="../configs", config_name="sam3d_body", version_base=None)
def main(cfg: DictConfig) -> None:
    # 1. è·¯å¾„å‡†å¤‡
    infer_type = cfg.infer.get("type", "video")
    out_root = Path(cfg.paths.log_path).resolve()
    infer_root = Path(cfg.paths.result_output_path).resolve()
    source_root = Path(
        cfg.paths.video_path if infer_type == "video" else cfg.paths.image_path
    ).resolve()

    gpu_ids = cfg.infer.get("gpu_ids", [0, 1])  # ä»é…ç½®æ–‡ä»¶è¯»å– GPU åˆ—è¡¨ï¼Œé»˜è®¤ [0, 1]

    all_person_dirs = sorted([x for x in source_root.iterdir() if x.is_dir()])
    if not all_person_dirs:
        logger.error(f"æœªæ‰¾åˆ°æ•°æ®ç›®å½•: {source_root}")
        return

    # 2. è‡ªåŠ¨åˆ†ç»„é€»è¾‘ (Task Chunking)
    # å°†æ‰€æœ‰ç›®å½•åˆ†æˆ N ä»½ï¼ŒN ç­‰äº GPU çš„æ•°é‡
    num_gpus = len(gpu_ids)
    # ä½¿ç”¨ np.array_split å¯ä»¥ç¡®ä¿å³ä½¿é™¤ä¸å°½ï¼Œåˆ†é…ä¹Ÿå°½å¯èƒ½å‡åŒ€
    chunks = np.array_split(all_person_dirs, num_gpus)

    logger.info(f"æ£€æµ‹åˆ° {num_gpus} ä¸ª GPU: {gpu_ids}")
    for i, gpu_id in enumerate(gpu_ids):
        logger.info(f"  - GPU {gpu_id} åˆ†é…ä»»åŠ¡æ•°: {len(chunks[i])}")

    # 3. å¯åŠ¨å¹¶è¡Œè¿›ç¨‹
    cfg_dict = OmegaConf.to_container(cfg, resolve=True)
    mp.set_start_method("spawn", force=True)

    processes = []
    for i, gpu_id in enumerate(gpu_ids):
        person_list = chunks[i].tolist()  # è½¬å›æ™®é€šåˆ—è¡¨
        if not person_list:
            continue

        p = mp.Process(
            target=gpu_worker,
            args=(
                gpu_id,
                person_list,
                source_root,
                out_root,
                infer_root,
                cfg_dict,
                infer_type,
            ),
        )
        p.start()
        processes.append(p)

    # 4. ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    for p in processes:
        p.join()

    logger.info("ğŸ‰ [SUCCESS] æ‰€æœ‰ GPU ä»»åŠ¡å·²åœ†æ»¡å®Œæˆï¼")


if __name__ == "__main__":
    os.environ["HYDRA_FULL_ERROR"] = "1"
    main()
